import axios, { CancelTokenSource } from "axios";
import { encode } from "gpt-3-encoder";
export type ChatOptions = {
  /**
   * An array of chat context objects or a string representing the current conversation context.
   *
   * @example
   * // Using an array of ChatContext objects
   * const chatContext = [
   *   { role: 'user', content: 'Hi, can you recommend a good restaurant?' },
   *   { role: 'assistant', content: 'Sure, what type of cuisine are you in the mood for?' },
   *   { role: 'user', content: 'I am in the mood for Italian food.' }
   * ];
   *
   * // Using a string to represent the chat context
   * const chatContext = "Hi, can you recommend a good restaurant?"
   */
  chatContext: ChatContext[] | string;
  /**
   * Executed before the start of a conversation, it returns a boolean value.
   * If the value is true, the conversation continues. If the value is false,
   * the subsequent content is interrupted. It is commonly used in scenarios
   * such as conditional verification and authentication.
   *
   * @example
   * const chatOptions: ChatOptions = {
   *   beforeReplyStart: (tokens) => {
   *     const isAuthenticated = checkUserAuthentication();
   *     if (!isAuthenticated) {
   *       console.log('User is not authenticated. Ending conversation.');
   *       return isAuthenticated;
   *     }
   *     if (tokens >= 5470) {
   *       console.log('Too long. Ending conversation.');
   *       return false
   *     }
   *     return true
   *   }
   * };
   */
  beforeReplyStart?: (promptTokens: number) => boolean;
  /**
   * Function to be called when the model starts generating a response.
   *
   * @example
   * const chatOptions: ChatOptions = {
   *   onReplyStart: (id: string) => {
   *     console.log(`Starting to generate reply for conversation with ID: ${id}`);
   *   }
   * };
   */
  onReplyStart?: (id: string) => void;
  /**
   * Function to be called when the model is generating a response. The reply parameter
   * is the current completion generated by the model.
   *
   * @example
   * const chatOptions: ChatOptions = {
   *   onReplying: (word: string, reply: string, id: string) => {
   *     console.log(`Current reply for conversation with ID ${id}: ${reply}`);
   *   }
   * };
   */
  onReplying?: (word: string, reply: string, id: string) => void;
  /**
   * Function to be called when the model finishes generating a response. The completion
   * parameter is the final generated response.
   *
   * @example
   * const chatOptions: ChatOptions = {
   *   onReplyEnd: (completion: string, id: string) => {
   *     console.log(`Generated reply for conversation with ID ${id}: ${completion}`);
   *   }
   * };
   */
  onReplyEnd?: (completion: string, id: string, usage: ChatUsage) => void;
  /**
   * Function to be called when an error occurs. The error parameter is the error object.
   */
  onError?: (error: any) => void;
  /**
   *This callback function is executed when the chat is cancelled.
   *
   *It returns the chat ID and usage information.
   *@example
   *const handleCancel = (id: string, usage: ChatUsage) => {
   *  console.log('Chat with ID ${id} has been cancelled.')
   *  console.log('Chat usage information:', usage)
   *}
   *
   */
  onCancel?: (id: string, usage: ChatUsage) => void;
  /** Additional configuration options to be passed to the model. */
  chatConfig?: ChatConfig;
};
export type ChatUsage = {
  /**  The number of total tokens used*/
  totalTokens: number;
  /**  The number of requests made to the API */
  promptTokens: number;
  /**  The number of tokens generated by the model */
  completionTokens: number;
};
export type ChatConfig = {
  /**  What sampling temperature to use, between 0 and 2 */
  temperature?: number;
  /**  An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass */
  top_p?: number;
  /**  How many chat completion choices to generate for each input message */
  n?: number;
  /**  Up to 4 sequences where the API will stop generating further tokens */
  stop?: string | string[];
  /**  The maximum number of tokens to generate in the chat completion */
  max_tokens?: number;
  /**  Number between -2.0 and 2.0 */
  presence_penalty?: number;
  /**  Number between -2.0 and 2.0 */
  frequency_penalty?: number;
  /**  A json object that maps tokens to an associated bias value from -100 to 100 */
  logit_bias?: { [key: string]: number };
  /**  A unique identifier representing your end-user */
  user?: string;
};

export type ChatContext = {
  role: "assistant" | "user" | "system";
  content: string;
};

class AwesomeChatBot {
  private API_URL: string;
  private token: string;
  private systemDescription?: string;
  private cancelTokenSource?: CancelTokenSource;
  private onCancel?: (id: string, usage: ChatUsage) => void;
  private chatUsage?: ChatUsage = {
    totalTokens: 0,
    promptTokens: 0,
    completionTokens: 0,
  };
  private id?: string;
  private response: string = "";

  constructor(token: string, systemDescription?: string) {
    this.token = token;

    systemDescription && (this.systemDescription = systemDescription);

    this.API_URL = token.startsWith("ac-")
      ? `http://api.awesomechat.cn/v1/chat/completions`
      : "https://api.openai.com/v1/chat/completions";
  }

  private clear() {
    this.cancelTokenSource = undefined;
    this.onCancel = undefined;
    this.response = "";
    this.chatUsage = {
      totalTokens: 0,
      promptTokens: 0,
      completionTokens: 0,
    };
    this.id = undefined;
  }

  private calculateUsage(prompt: string, reply: string): ChatUsage {
    const promptTokens = encode(prompt).length;
    const completionTokens = encode(reply).length;
    const usage: ChatUsage = {
      totalTokens: promptTokens + completionTokens,
      promptTokens,
      completionTokens,
    };
    return usage;
  }

  public cancel() {
    if (!this.id) return;

    this.cancelTokenSource!.cancel("Canceling the chat request");
    this.onCancel?.(this.id!, this.chatUsage!);
  }

  public async chat({
    chatContext,
    beforeReplyStart,
    onReplyStart,
    onReplying,
    onReplyEnd,
    chatConfig,
    onError,
    onCancel,
  }: ChatOptions) {
    try {
      if (!chatContext) {
        throw new Error("chatContext is required");
      }
      const messages = [
        ...(this.systemDescription
          ? [{ role: "system", content: this.systemDescription }]
          : []),
        ...(typeof chatContext === "string"
          ? [{ role: "user", content: chatContext }]
          : chatContext),
      ];

      const tokensToSend = messages.reduce((acc, item) => {
        return acc + encode(item.content).length;
      }, 0);

      const next = beforeReplyStart?.(tokensToSend);
      if (next === false) return;
      this.clear();

      this.cancelTokenSource = axios.CancelToken.source();
      this.onCancel = onCancel;

      const request = {
        model: "gpt-3.5-turbo",
        messages,
        stream: true,
        ...chatConfig,
      };

      const promptString = messages.map((item) => item.content).join(" ");

      const { data } = await axios.post(this.API_URL, request, {
        headers: {
          Authorization: `Bearer ${this.token}`,
        },
        responseType: "stream",
        cancelToken: this.cancelTokenSource.token,
      });

      data.on("data", (chunk: any) => {
        try {
          if (!chunk.toString().includes("data")) throw new Error(chunk);
          const node = chunk.toString().replaceAll("data: ", "");
          let nodeArr = node
            .split("\n")
            .filter((item: string) => item !== "" && item !== "[DONE]");

          if (!Array.isArray(nodeArr)) nodeArr = [node];

          nodeArr.forEach((item: string) => {
            if (!this.id) {
              this.id = JSON.parse(item).id;
              onReplyStart?.(this.id!);
            }

            const word = JSON.parse(item).choices[0].delta.content || "";
            if (word) {
              this.response += word;
            }

            onReplying?.(word, this.response, this.id!);
            this.chatUsage = this.calculateUsage(promptString, this.response);
          });
        } catch (error: any) {}
      });

      data.on("end", () => {
        const usage = this.calculateUsage(promptString, this.response);
        this.chatUsage = usage;
        this.id && onReplyEnd?.(this.response, this.id!, this.chatUsage);
      });

      data.on("error", (error: any) => {
        onError?.(error);
      });
    } catch (error: any) {
      onError?.(error);
    }
  }
}

export default AwesomeChatBot;
